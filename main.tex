\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{\textbf{Text Summarizer for Amazon Food Reviews}\\}
\author{\textbf{\Large{Yangxiao Bai, Chen-Wei Hung, XiaoZhu Jin, Shi Wen Wong}} \\
South Dakota State University \\
\{bai.yangxiao, chenwei.hung, xiaozhu.jin, shiwen.wong\}@jacks.sdstate.edu \\
}

\maketitle

\begin{abstract}
Online shopping has become a mainstream method of making purchases due to its convenience and efficiency. Reviews play a significant role in assisting shoppers to make purchasing decisions and benefit entrepreneurs by building trust, setting their brand apart from others, and allowing them to make improvements based on the reviews. This project focuses on building a text summarizer for Amazon reviews of fine foods by using Bidirectional Recurrent Neural Network (RNN) with an attention mechanism. The goal of this work is to shorten the length of reviews and eliminate irrelevant information while preserving the original context and tone, and ultimately, to save buyers’ time and allow informed purchasing decisions in a timely manner. \\
\end{abstract} 

\begin{IEEEkeywords}
Text Summarizer, Bidirectional Recurrent Neural Network (RNN), Attention Mechanism, Long-Short Term Memory (LSTM)
\end{IEEEkeywords}

\section{Introduction}
A Recurrent Neural Network (RNN) was introduced in 1986 to process sequential data using feedback loops, and it is commonly used for Natural Language Processing related tasks such as language translation, speech recognition, text prediction, and video captioning\cite{LSTM}. One drawback RNN experiences is that gradients vanish during backward propagation resulting in the model stopping to learn. In 1997, Long Short-Term Memory (LSTM) was designed by Sepp Hochreiter and J\"{u}rgen Schmidhuber to overcome the gradient-vanishing problem. Furthermore, unlike a regular RNN, LSTM is capable of learning long-term dependencies effectively through several gates that control what pieces of information should be passed along or discarded. \\ \\
\indent In general, when summarizing a paragraph or a sentence, the meaning of a word is related to the content that precedes and follows it. The Bidirectional RNN works well here, as it can access the inputs from past and future time steps. Additionally, an attention mechanism will be added to the encoding layer to determine which words in the sentence are the keywords before sending them to the decoding layer\cite{AIAYN}. As a result, it boosts the accuracy of the text summarizer.

\section{Goals and Objectives}
content

\section{Related Work and Preliminary Results}
content 
\subsection{RNN}
content
\subsection{LSTM}
content
\subsection{Text summarizer}
content

\section{Research Plan}
\subsection{Preprocess}
\subsection{Model}

\section{Evaluation Plan}
content \\

\begin{thebibliography}{1,2}
\bibitem{LSTM}
Ralf, C., Staudemeyer., Eric, Rothstein, Morris. (2019). Understanding LSTM -- a tutorial into Long Short-Term Memory Recurrent Neural Networks. arXiv: Neural and Evolutionary Computing.
\bibitem{AIAYN}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł. and Polosukhin, I. (2017). Attention is all you need. arXiv: Computation and Language.
\end{thebibliography}

\end{document}
